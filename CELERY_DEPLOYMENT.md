# Celery Distributed Worker System - Deployment Guide

## ğŸ“‹ Genel BakÄ±ÅŸ

Bu mimari, **1M+ Ã¼rÃ¼n** iÃ§in optimize edilmiÅŸ production-ready distributed task queue sistemidir.

### Celery Distributed Task Queue System

**Production-ready sistem Ã¶zellikleri:**
- âœ… Distributed task queue (Redis)
- âœ… 10-100+ worker desteÄŸi
- âœ… 1M+ Ã¼rÃ¼n kapasitesi
- âœ… Paralel iÅŸlem (80-400+ concurrent tasks)
- âœ… Priority-based scheduling
- âœ… Auto-retry mechanism
- âœ… Flower monitoring dashboard
- âœ… Horizontal scaling

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Database Migration

Ã–nce yeni alanlarÄ± ekleyin:

```bash
# PostgreSQL'e baÄŸlan
psql -U fiyatradari -d fiyatradari

# Migration'Ä± Ã§alÄ±ÅŸtÄ±r
\i backend/migrations/add_celery_fields.sql
```

### 2. Dependencies Kurulumu

```bash
cd worker
pip install -r requirements.txt
```

### 3. Environment Variables

`.env` dosyanÄ±za ekleyin:

```bash
# Redis Configuration
REDIS_URL=redis://localhost:6379
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/1
```

### 4. Sistemi BaÅŸlatma

#### Option A: Docker Compose (Ã–nerilen)

```bash
# TÃ¼m servisleri baÅŸlat
docker-compose up -d

# Flower monitoring: http://localhost:5555
```

#### Option B: Manuel BaÅŸlatma

```bash
# Terminal 1: Redis
redis-server

# Terminal 2: Celery Worker Pool (3 worker, her biri 4 concurrent task)
cd worker
celery -A celery_app worker --loglevel=info --concurrency=4

# Terminal 3: Celery Beat (Scheduler)
cd worker
celery -A celery_app beat --loglevel=info

# Terminal 4: Flower (Monitoring) - Opsiyonel
cd worker
celery -A celery_app flower --port=5555

```

## ğŸ“Š Monitoring

### Flower Dashboard

Web arayÃ¼zÃ¼: http://localhost:5555

**Ã–zellikler:**
- Active tasks gÃ¶rÃ¼ntÃ¼leme
- Worker durumlarÄ±
- Task baÅŸarÄ±/hata oranlarÄ±
- Task execution time grafikleri
- Real-time monitoring

### Celery CLI Commands

```bash
# Worker'larÄ± gÃ¶rÃ¼ntÃ¼le
celery -A celery_app inspect active

# Stats
celery -A celery_app inspect stats

# Registered tasks
celery -A celery_app inspect registered

# Queue'larÄ± gÃ¶rÃ¼ntÃ¼le
celery -A celery_app inspect active_queues

# Task'Ä± iptal et
celery -A celery_app control revoke <task_id>

# Worker'Ä± durdur
celery -A celery_app control shutdown
```

## ğŸ¯ Task Ã‡eÅŸitleri ve Zamanlama

### Otomatik ZamanlanmÄ±ÅŸ Task'lar (Celery Beat)

| Task | SÄ±klÄ±k | AÃ§Ä±klama |
|------|--------|----------|
| `schedule_high_priority_checks` | Her saat | Active deal'leri olan Ã¼rÃ¼nler |
| `schedule_medium_priority_checks` | Her 6 saat | Orta Ã¶ncelik Ã¼rÃ¼nler |
| `schedule_low_priority_checks` | GÃ¼nlÃ¼k (03:00) | DÃ¼ÅŸÃ¼k Ã¶ncelik Ã¼rÃ¼nler |
| `schedule_product_fetch` | GÃ¼nlÃ¼k (04:00) | Yeni Ã¼rÃ¼n Ã§ekme |
| `schedule_notifications` | Her 30 dakika | Telegram bildirimleri |
| `update_product_priorities` | Her 4 saat | Priority score gÃ¼ncelleme |
| `cleanup_old_data` | GÃ¼nlÃ¼k (02:00) | Eski veri temizleme |

### Manuel Task Tetikleme

Python'dan:

```python
from celery_tasks import check_product_price, batch_price_check

# Single product
result = check_product_price.delay(product_id=123, priority=10)
print(result.id)  # Task ID

# Batch
result = batch_price_check.delay([1, 2, 3, 4, 5], priority=8)
```

Celery CLI'dan:

```bash
# Task tetikle
celery -A celery_app call celery_tasks.check_product_price --args='[123]' --kwargs='{"priority": 10}'
```

## ğŸ”§ KonfigÃ¼rasyon

### Worker Scaling

#### Docker Compose ile:

```yaml
# docker-compose.yml'de replicas deÄŸiÅŸtir
celery_worker:
  deploy:
    replicas: 10  # 10 worker instance
```

#### Manuel:

```bash
# Concurrency artÄ±r (her worker'da daha fazla task)
celery -A celery_app worker --concurrency=20

# Birden fazla worker baÅŸlat
celery -A celery_app worker -n worker1@%h --concurrency=4
celery -A celery_app worker -n worker2@%h --concurrency=4
celery -A celery_app worker -n worker3@%h --concurrency=4
```

### Queue Priorities

`celery_app.py`'de queue tanÄ±mlarÄ±:

```python
task_queues=(
    Queue('price_check', ..., queue_arguments={'x-max-priority': 10}),
    Queue('product_fetch', ..., queue_arguments={'x-max-priority': 5}),
    Queue('notifications', ..., queue_arguments={'x-max-priority': 8}),
    Queue('batch_processing', ..., queue_arguments={'x-max-priority': 3}),
)
```

### Rate Limiting

Her task iÃ§in rate limit:

```python
@app.task(rate_limit='100/m')  # 100 tasks per minute
def check_product_price(product_id):
    pass
```

## ğŸ“ˆ Priority System

### Priority Hesaplama

Product priority (0-100) ÅŸu faktÃ¶rlere gÃ¶re hesaplanÄ±r:

```python
Priority = (
    has_active_deal * 50 +      # Active deal varsa 50 puan
    volatility_score * 0.30 +   # Fiyat volatility %30
    popularity_score * 0.15 +   # PopÃ¼lerlik %15
    recency_score * 0.05        # Son kontrol zamanÄ± %5
)
```

### Check Intervals (Priority'ye gÃ¶re)

- **Priority >= 80:** Her saat kontrol
- **Priority 60-79:** Her 3 saat
- **Priority 40-59:** Her 6 saat
- **Priority 20-39:** Her 12 saat
- **Priority < 20:** GÃ¼nde 1

### Priority GÃ¼ncelleme

Otomatik (her 4 saatte):
```python
# Celery Beat tarafÄ±ndan otomatik Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r
update_product_priorities()
```

Manuel:
```python
from services.priority_calculator import PriorityCalculator
calculator = PriorityCalculator()

with get_db() as db:
    product = db.query(Product).first()
    priority = calculator.calculate_priority(product, db)
    product.check_priority = priority
    db.commit()
```

## ğŸ” Smart Batching

### Batch Processor KullanÄ±mÄ±

```python
from services.smart_batch_processor import SmartBatchProcessor

processor = SmartBatchProcessor(batch_size=1000)

# High priority batch'ler al
batches = processor.get_high_priority_batches(limit=10000)
for batch in batches:
    batch_price_check.delay(batch['product_ids'], priority=10)

# Statistics
stats = processor.get_statistics()
print(stats)
```

### Batch Stratejisi

1. **High Priority (her saat)**
   - Active deal'leri olan Ã¼rÃ¼nler
   - Priority score >= 80
   - Son 1 saatte kontrol edilmemiÅŸ

2. **Medium Priority (her 6 saat)**
   - Priority score 40-79
   - Son 6 saatte kontrol edilmemiÅŸ

3. **Low Priority (gÃ¼nlÃ¼k)**
   - Priority score < 40
   - Son 24 saatte kontrol edilmemiÅŸ

## ğŸ› Debugging

### Task SonuÃ§larÄ±nÄ± GÃ¶rme

```python
from celery.result import AsyncResult

result = AsyncResult('task-id-here')
print(result.state)  # PENDING, STARTED, SUCCESS, FAILURE
print(result.result)  # Task sonucu
print(result.traceback)  # Hata varsa traceback
```

### LoglarÄ± Ä°zleme

```bash
# Worker logs
docker-compose logs -f celery_worker

# Beat logs
docker-compose logs -f celery_beat

# TÃ¼m Celery loglarÄ±
docker-compose logs -f celery_worker celery_beat flower
```

### Common Issues

#### 1. "Connection refused" hatasÄ±
```bash
# Redis Ã§alÄ±ÅŸÄ±yor mu?
redis-cli ping  # PONG dÃ¶nmeli

# Docker'da
docker-compose ps redis
```

#### 2. Task'lar Ã§alÄ±ÅŸmÄ±yor
```bash
# Worker'lar aktif mi?
celery -A celery_app inspect active_queues

# Beat Ã§alÄ±ÅŸÄ±yor mu?
docker-compose logs celery_beat | grep "Scheduler:"
```

#### 3. YavaÅŸ performans
```bash
# Worker sayÄ±sÄ±nÄ± artÄ±r
docker-compose up -d --scale celery_worker=10

# Veya concurrency artÄ±r
celery -A celery_app worker --concurrency=20
```

## âœ… Deployment Checklist

- [ ] Database migration Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±
- [ ] Redis yÃ¼klendi ve Ã§alÄ±ÅŸÄ±yor
- [ ] Dependencies kuruldu (`pip install -r requirements.txt`)
- [ ] Environment variables eklendi
- [ ] Celery worker baÅŸlatÄ±ldÄ± ve Ã§alÄ±ÅŸÄ±yor
- [ ] Celery beat baÅŸlatÄ±ldÄ±
- [ ] Flower monitoring eriÅŸilebilir
- [ ] Ä°lk batch test edildi
- [ ] Priority'ler gÃ¼ncellendi
- [ ] 24 saat monitoring yapÄ±ldÄ±

## ğŸ“Š Performance Metrics

### 1M ÃœrÃ¼n iÃ§in Beklenen Performans

| Metrik | DeÄŸer |
|--------|-------|
| **Worker sayÄ±sÄ±** | 10-20 |
| **Concurrent tasks** | 80-400 |
| **ÃœrÃ¼n/saat** | 150K-300K |
| **Full check sÃ¼resi** | 3-7 saat |
| **Memory usage** | ~2-4 GB total |
| **Redis memory** | ~500 MB |

### Monitoring Metrikleri

Flower'da takip edilmesi gerekenler:
- Task success rate (>95% olmalÄ±)
- Average task duration (<30 saniye olmalÄ±)
- Worker utilization (70-90% ideal)
- Queue length (sÃ¼rekli artmamalÄ±)

## ğŸš¨ Production Deployment

### Kubernetes (Opsiyonel)

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: celery-worker
spec:
  replicas: 20  # 20 worker pod
  template:
    spec:
      containers:
      - name: worker
        image: fiyatradari-worker:latest
        command: ["celery", "-A", "celery_app", "worker"]
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
```

### Auto-scaling Setup

```python
# autoscaler.py
from celery.app.control import Control

def scale_workers(queue_length):
    """Auto-scale based on queue length"""
    if queue_length > 10000:
        # Scale up
        os.system("docker-compose up -d --scale celery_worker=20")
    elif queue_length < 1000:
        # Scale down
        os.system("docker-compose up -d --scale celery_worker=5")
```

## ğŸ“š Ek Kaynaklar

- [Celery Documentation](https://docs.celeryproject.org/)
- [Redis Best Practices](https://redis.io/topics/optimization)
- [Flower Monitoring](https://flower.readthedocs.io/)

## ğŸ¤ Destek

Sorun yaÅŸarsanÄ±z:
1. Flower dashboard'u kontrol edin: http://localhost:5555
2. Worker loglarÄ±nÄ± inceleyin: `docker-compose logs celery_worker`
3. Redis baÄŸlantÄ±sÄ±nÄ± test edin: `redis-cli ping`
4. Statistics'leri kontrol edin:
   ```python
   from services.smart_batch_processor import SmartBatchProcessor
   print(SmartBatchProcessor().get_statistics())
   ```
